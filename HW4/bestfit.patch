diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index b3560ec..d77d323 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -365,3 +365,5 @@
 356	i386	memfd_create		sys_memfd_create
 357	i386	bpf			sys_bpf
 358	i386	execveat		sys_execveat			stub32_execveat
+359	i386	slob_used		sys_total_used
+360	i386	slob_free		sys_total_freed
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 85893d7..5364aa7 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -881,5 +881,6 @@ asmlinkage long sys_bpf(int cmd, union bpf_attr *attr, unsigned int size);
 asmlinkage long sys_execveat(int dfd, const char __user *filename,
 			const char __user *const __user *argv,
 			const char __user *const __user *envp, int flags);
-
+asmlinkage long sys_total_used(void);
+asmlinkage long sys_total_freed(void);
 #endif
diff --git a/mm/slob.c b/mm/slob.c
index 96a8620..aa15d48 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -68,6 +68,8 @@
 #include <linux/list.h>
 #include <linux/kmemleak.h>
 
+#include <linux/syscalls.h>
+#include <linux/linkage.h>
 #include <trace/events/kmem.h>
 
 #include <linux/atomic.h>
@@ -87,6 +89,10 @@ typedef s16 slobidx_t;
 typedef s32 slobidx_t;
 #endif
 
+unsigned long page_count_slob = 0;
+unsigned long free_units = 0;
+pgoff_t total;
+
 struct slob_block {
 	slobidx_t units;
 };
@@ -268,11 +274,15 @@ static void *slob_page_alloc(struct page *sp, size_t size, int align)
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct page *sp;
+	struct page *bf = NULL;
 	struct list_head *prev;
 	struct list_head *slob_list;
 	slob_t *b = NULL;
 	unsigned long flags;
+	pgoff_t start;
+	pgoff_t end;
 
+	free_units = 0;
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
 	else if (size < SLOB_BREAK2)
@@ -294,10 +304,17 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		/* Enough room on this page? */
 		if (sp->units < SLOB_UNITS(size))
 			continue;
+		if(bf == NULL)
+			bf = sp;
+		if(sp->units < bf->units)
+			bf = sp;
+		if((bf->units == SLOB_UNITS(size)) || (sp->lru.next == NULL));
+		else
+			continue;
 
 		/* Attempt to alloc */
-		prev = sp->lru.prev;
-		b = slob_page_alloc(sp, size, align);
+		prev = bf->lru.prev;
+		b = slob_page_alloc(bf, size, align);
 		if (!b)
 			continue;
 
@@ -309,6 +326,17 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 			list_move_tail(slob_list, prev->next);
 		break;
 	}
+	//get the amount of free space
+	list_for_each_entry(sp, &free_slob_small, lru){
+		free_units = free_units + sp->units;
+	}
+	list_for_each_entry(sp, &free_slob_medium, lru){
+		free_units = free_units + sp->units;
+	}
+	list_for_each_entry(sp, &free_slob_large, lru){
+		free_units = free_units + sp->units;
+	}
+
 	spin_unlock_irqrestore(&slob_lock, flags);
 
 	/* Not enough space: must allocate a new page */
@@ -328,9 +356,31 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
+		page_count_slob++;
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
+
+	//get total space used
+	/*start = sp->freelist;
+	list_for_each_entry(sp, &free_slob_small, lru){
+		end = sp->index;
+	}
+	total = (end + (page_count_slob-1)*PAGE_SIZE) - start;
+
+	start = sp->freelist;
+	list_for_each_entry(sp, &free_slob_medium, lru){
+		end = sp->index;
+	}
+	total += (end + (page_count_slob-1)*PAGE_SIZE) - start;
+
+	start = sp->freelist;
+	list_for_each_entry(sp, &free_slob_large, lru){
+		end = sp->index;
+	}
+	total += (end + (page_count_slob-1)*PAGE_SIZE) - start;*/
+
+
 	return b;
 }
 
@@ -362,6 +412,7 @@ static void slob_free(void *block, int size)
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+		page_count_slob--;
 		return;
 	}
 
@@ -630,6 +681,16 @@ struct kmem_cache kmem_cache_boot = {
 	.align = ARCH_KMALLOC_MINALIGN,
 };
 
+asmlinkage long sys_total_used(void) {
+   long total_used = SLOB_UNITS(PAGE_SIZE) * page_count_slob;
+
+   return total_used;
+}
+
+asmlinkage long sys_total_freed(void){
+   return free_units;
+}
+
 void __init kmem_cache_init(void)
 {
 	kmem_cache = &kmem_cache_boot;
