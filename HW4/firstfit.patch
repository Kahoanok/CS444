diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index b3560ec..d77d323 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -365,3 +365,5 @@
 356	i386	memfd_create		sys_memfd_create
 357	i386	bpf			sys_bpf
 358	i386	execveat		sys_execveat			stub32_execveat
+359	i386	slob_used		sys_total_used
+360	i386	slob_free		sys_total_freed
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 85893d7..5364aa7 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -881,5 +881,6 @@ asmlinkage long sys_bpf(int cmd, union bpf_attr *attr, unsigned int size);
 asmlinkage long sys_execveat(int dfd, const char __user *filename,
 			const char __user *const __user *argv,
 			const char __user *const __user *envp, int flags);
-
+asmlinkage long sys_total_used(void);
+asmlinkage long sys_total_freed(void);
 #endif
diff --git a/mm/slob.c b/mm/slob.c
index 96a8620..3411f30 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -87,6 +87,8 @@ typedef s16 slobidx_t;
 typedef s32 slobidx_t;
 #endif
 
+unsigned long page_count_slob = 0;
+unsigned long free_units = 0;
 struct slob_block {
 	slobidx_t units;
 };
@@ -272,7 +274,7 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 	struct list_head *slob_list;
 	slob_t *b = NULL;
 	unsigned long flags;
-
+	free_units = 0;
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
 	else if (size < SLOB_BREAK2)
@@ -309,6 +311,15 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 			list_move_tail(slob_list, prev->next);
 		break;
 	}
+	list_for_each_entry(sp,&free_slob_small,lru){
+		free_units = free_units + sp->units;
+	}
+	list_for_each_entry(sp,&free_slob_medium,lru){
+		free_units = free_units + sp->units;
+	}
+	list_for_each_entry(sp,&free_slob_large,lru){
+		free_units = free_units + sp->units;
+	}
 	spin_unlock_irqrestore(&slob_lock, flags);
 
 	/* Not enough space: must allocate a new page */
@@ -328,6 +339,7 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
+		page_count_slob++;
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
@@ -362,6 +374,7 @@ static void slob_free(void *block, int size)
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+		page_count_slob--;
 		return;
 	}
 
@@ -629,6 +642,14 @@ struct kmem_cache kmem_cache_boot = {
 	.flags = SLAB_PANIC,
 	.align = ARCH_KMALLOC_MINALIGN,
 };
+asmlinkage long sys_total_used(void){
+	long total_used = SLOB_UNITS(PAGE_SIZE) * page_count_slob;
+
+	return total_used;
+}
+asmlinkage long sys_total_freed(void){
+	return free_units;
+}
 
 void __init kmem_cache_init(void)
 {
@@ -640,3 +661,4 @@ void __init kmem_cache_init_late(void)
 {
 	slab_state = FULL;
 }
+
